{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 13:50:01.956429: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-09 13:50:02.473179: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-09 13:50:03.021105: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.051046: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.051194: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.052522: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.052633: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.052720: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.369348: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.369475: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.369578: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-07-09 13:50:03.369644: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:227] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-07-09 13:50:03.369686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21436 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0a:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "from random import randint\n",
    "import numpy as np\n",
    "gpu = tf.config.list_physical_devices('GPU')[0]\n",
    "tf.config.experimental.set_memory_growth(gpu, True)\n",
    "import os\n",
    "os.environ[\"TF_GPU_ALLOCATOR\"] = \"cuda_malloc_async\"\n",
    "\n",
    "datafiles = \"/home/ericjm24/fimarch/txt/*.txt\"\n",
    "base_dataset = tf.data.TextLineDataset(tf.data.Dataset.list_files(datafiles))\n",
    "base_dataset = base_dataset.map(lambda x: tf.strings.regex_replace(x, r'''([!(),.?\\-'\";:])''', r' \\1 '))\n",
    "base_dataset = base_dataset.map(lambda x: tf.strings.regex_replace(x, r'''[^ a-z0-9A-Z!(),.?\\-'\";:]''', ''))\n",
    "base_dataset = base_dataset.map(lambda x: tf.strings.regex_replace(x, r'''(.*)''', r'startstorystart \\1 endstoryend'))\n",
    "base_dataset = base_dataset.map(lambda x: tf.strings.regex_replace(x, r'\\s+', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = 10000,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "REBUILD_VOCAB = False\n",
    "\n",
    "if REBUILD_VOCAB == True:\n",
    "    vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "        base_dataset.batch(batch_size = 5, num_parallel_calls=tf.data.AUTOTUNE).prefetch(10),\n",
    "        **bert_vocab_args\n",
    "    )\n",
    "\n",
    "    def write_vocab_file(filepath, vocab):\n",
    "      with open(filepath, 'w') as f:\n",
    "        for token in vocab:\n",
    "          print(token, file=f)\n",
    "\n",
    "    write_vocab_file('vocab.txt', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf_text.BertTokenizer('vocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'startstorystart Three days . Rarity hadn \\' t slept in three days . Three days of coffee , take - out containers and screaming . That \\' s how long it took Sweetie Belle to finally call for help . \" - - And she hasn \\' t opened the door to her Inspiration Room since , \" the freshman concluded , having tried to catch the others up to what had been going on . Applejack nodded grimly . \" So she didn \\' t say who this mystery client was ? \" Arms crossed , she radiated both concern and being already fed up with the drama . Not an unusual response to Rarity \\' s antics . Sweetie \\' s nod in response was much more rapid . \" Yeah , though I think I recognized the voice when they were on speakerphone before . It sounded like Vignette Valencia . \" There was a growl from around waist - level . Sunset looked up from her work . \" Great . So it \\' s possible Vignette found another artifact or we didn \\' t completely clean her last time , and she \\' s infected Rarity with Equestrian magic . \" Her eyes turned back to the doorknob . An apple - heavy fist banged on the door . \" Ya hear that , Rarity ? We \\' re just concerned is all . Now open up ! If ya don \\' t before Sunset finishes picking this lock , Ah ain \\' t - - \" Almost on cue , there was a click and the latch released . Leaving a fretting Sweetie behind , the two Rainbooms entered . It had been decided to go gentle at the start with just the two of them - this wasn \\' t the first time Rarity had driven herself too hard , and a full group confrontation might just make things worse . Or encourage her to be even more over - dramatic . The Inspiration Room was a crime scene . Every surface that wasn \\' t polluted by empty latte cups and takeout boxes was infected by a mess of materials . Not just fabric of all sorts but metal and cardboard and plastic and rubber and a few things that were less identifiable . Four separate mannequins had been overwhelmed by the chaos . One had been overwhelmed by garbage . All of them were likely bound for Rarity \\' s Closet of Shame once this was all over . And in the middle of it all was Rarity . Gently vibrating from the amount of caffeine in her , red glasses holding back weary eyeballs and heavy bags , and hands still working as she desperately tried to re - thread a needle . Door closing behind them ( mostly - Sweetie Belle kept it open a crack so she could peek through ) , Sunset and Applejack carefully stepped through the chaos to opposite sides of their friend . One hesitated - the other didn \\' t . \" Rarity , \" Applejack drawled . No response . \" RARITY , \" she tried louder . Again , nothing . This time , the farmgirl reached out to grab a pale shoulder . \" RARITY ! \" The puppet \\' s strings were cut , and Rarity slumped into Applejack \\' s arms with a cry of despair . Without hesitation , Applejack pulled Rarity close . She held her friend tight , letting the fashionista weep on a flannel - insulated shoulder . \" GARBAGE ! \" Rarity \\' s wail exploded out in frustration . \" It \\' s all ga - a - a - arba - a - a - a - ge ! \" \" Shh . It \\' s alright , sugarcube . \" Gently , Applejack stroked Rarity \\' s hair . \" It \\' s alright . Ah \\' m sure you just need to take a breath an \\' you \\' ll find it ain \\' t that bad . \" Rarity \\' s head popped up like a mascara - stained gopher . \" No darling , I mean it \\' s literally garbage . \" From off to the side , Sunset spoke up . \" She \\' s not joking , Applejack . \" She was leaning in close to one of the mannequins , trying to get a better look at the work . \" There \\' s some markings on the inside of the fabric on this one . I think it \\' s - - \" \" Repurposed from old grocery bags , yes . \" Rarity cut in - right before breaking down again . \" It \\' s all tra - a - a - a - a - ash ! \" Applejack rolled her eyes . \" Get ahold of yerself , Rares , or I swear to sugar I \\' ll let go of ya . \" The threat was enough , it seemed . Rarity clamped her mouth shut and tried to maintain some measure of composure . Teary eyes looked to her friends - friends who were obviously demanding an explanation of what had happened . \" Vignette called me , \" she began , \" With a few new pieces she wanted . I - - I think she feels bad about what happened and she \\' s trying to make it up to me in her own way . \" Rarity paused to dab her ruined makeup with a handkerchief that appeared from nowhere . \" The theme is that she wanted something made entirely from recycled materials - she \\' s convinced that she \\' s going to make that a new hot trend . It needs to be up to my usual standards , but conspicuously identifiable as reused as well . \" Sunset ran her hands over the one dress while giving it a critical eye . \" I don \\' t see what the problem is , I guess . All of these look pretty good to me . \" Instantly , Rarity scoffed . \" They \\' re hideous . But the greater problem is that they aren \\' t what Vignette wants . At least , not anymore . \" Letting out a deep sigh , she withdrew deeper into Applejack \\' s arms . \" Because she \\' s trying to create a trend , she \\' s been rather demanding . And by demanding I mean she can \\' t make up her mind ! First she wants to have all of the brand logos on the original materials visible , then she calls me back two hours later to say that won \\' t do . Then I get a text that she \\' s changed her mind and needs a pants suit rather than a dress . Then she switches from wanting heels to flats and wants to use this hideous recycled rubber ! \" \" . . . So she \\' s demanding , \" Sunset summarized . Rarity threw her hands into the air . \" Demanding ? ! She \\' s impossible ! I swear that woman has a sixth sense for knowing when I \\' m exactly halfway through something so she can call and request changes that make me start over . I \\' ve been at this for days and all I have to show for it is nearly a dozen half - completed pieces of clothing and an ugly pair of shoes . \" Strong apple - scented arms squeezed Rarity in a hug . \" Maybe you should take a break . Y \\' know , get yer head clear . Some sleep wouldn \\' t kill ya , now would it ? \" Sunset broke away from the mess of the dress . \" AJ \\' s right . You aren \\' t at your best right now , even if Vignette \\' s demands were reasonable . \" She picked up the aforementioned flats from the table and held them out to Rarity . They were , in a word , abominations . A horrible mess of recycled rubber that looked like someone had made Crocs for a car . \" I mean , look at these . You \\' re better than this , Rarity . You need rest . \" With a mixture of disgust and care , Rarity took the terrible shoes into her hands . Worn , tired eyes looked down at them . \" I wish I could , darlings . But I think this one may have beaten me . \" She sagged , letting out a sigh of defeat . \" If I rested , yes , I might be able to get the pants to work or find the proper material for the dress . But these ? \" She waggled the shoes in the air . \" Sleep doesn \\' t help if the soles are still tires . \" The prompt for this competition was \" Sleep doesn \\' t help if it \\' s your soul that \\' s tired . \" I could have written a serious , somber story about being psychologically worn down and depression . One full of drama and angst and deep thoughts . You \\' re getting a feghoot off the prompt instead . noregrets ( I may write a more proper entry later , depending on inspiration . ) endstoryend', shape=(), dtype=string)\n",
      "<tf.RaggedTensor [[[b'startstorystart'],\n",
      "  [b'three'],\n",
      "  [b'days'],\n",
      "  ...,\n",
      "  [b'.'],\n",
      "  [b')'],\n",
      "  [b'endstoryend']]]>\n"
     ]
    }
   ],
   "source": [
    "for item in base_dataset.take(1):\n",
    "    print(item)\n",
    "    print(tokenizer.detokenize(tokenizer.tokenize(item)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 400\n",
    "dataset = base_dataset.map(lambda x: tf.strings.split(x, ' '))\n",
    "dataset = dataset.map(lambda x: tf_text.sliding_window(x, WINDOW)).unbatch()\n",
    "dataset = dataset.map(lambda x: tf.strings.reduce_join(x, axis=0, separator = ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TOKENS = WINDOW\n",
    "def prepare_batch(words):\n",
    "    temp = tokenizer.tokenize(words).merge_dims(-2,-1)\n",
    "    input = temp[:,:MAX_TOKENS].to_tensor()\n",
    "    out = temp[:,1:(MAX_TOKENS+1)].to_tensor()\n",
    "    return (input,input),  out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 24\n",
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .prefetch(buffer_size = tf.data.AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24, 400)\n",
      "(24, 400)\n",
      "tf.Tensor([  31 1031  915 5949 5416   73   11 6654 2182 6503], shape=(10,), dtype=int64)\n",
      "tf.Tensor([1031  915 5949 5416   73   11 6654 2182 6503  318], shape=(10,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for inp, outp in make_batches(dataset).take(1):\n",
    "    inp = inp[0]\n",
    "    print(inp.shape)\n",
    "    print(outp.shape)\n",
    "print(inp[0][0:10])\n",
    "print(outp[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shard(43, 0).shuffle(BUFFER_SIZE)\n",
    "dsize = 500000\n",
    "#training = dataset.take(dsize)\n",
    "#validation = dataset.skip(dsize).take(int(dsize*0.2))\n",
    "validation = dataset.shard(num_shards=5, index = 0).take(int(dsize*0.25))\n",
    "training = dataset.sample_from_datasets([dataset.shard(num_shards=5, index = 1),dataset.shard(num_shards=5, index = 2),dataset.shard(num_shards=5, index = 3),dataset.shard(num_shards=5, index = 4)]).take(dsize)\n",
    "\n",
    "training_batches = make_batches(training)\n",
    "validation_batches = make_batches(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(length, depth):\n",
    "    depth = depth/2\n",
    "    positions = np.arange(length)[:,np.newaxis]\n",
    "    depths = np.arange(depth)[np.newaxis,:]/depth\n",
    "    angle_rads = positions * (1 / (10000 ** depths))\n",
    "\n",
    "    pos_encoding = np.concatenate(\n",
    "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
    "        axis = -1\n",
    "    )\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model, mask_zero = True)\n",
    "        self.pos_encoding = positional_encoding(length = 2048, depth = d_model)\n",
    "    \n",
    "    def compute_mask(self, *args, **kwargs):\n",
    "        return self.embedding.compute_mask(*args, **kwargs)\n",
    "    \n",
    "    def call(self, x):\n",
    "        length = tf.shape(x)[1]\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, dtype=tf.float32))\n",
    "        x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
    "        return x\n",
    "\n",
    "VOCAB_SIZE = 9831\n",
    "\n",
    "enc_layer = PositionalEncoding(VOCAB_SIZE, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, dropout_rate = 0.1, **kwargs):\n",
    "        super().__init__()\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "        self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "        self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttentionLayer(BaseAttentionLayer):\n",
    "    def call(self, x, context):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            key = context,\n",
    "            value = context\n",
    "        )\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GlobalSelfAttentionLayer(CrossAttentionLayer):\n",
    "    def call(self, x):\n",
    "        return super().call(x, x)\n",
    "\n",
    "class CausalSelfAttentionLayer(BaseAttentionLayer):\n",
    "    def call(self, x):\n",
    "        attn_output = self.mha(\n",
    "            query = x,\n",
    "            key = x,\n",
    "            value = x,\n",
    "            use_causal_mask = True\n",
    "        )\n",
    "\n",
    "        x = self.add([x, attn_output])\n",
    "        x = self.layernorm(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.seq = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation = 'relu'),\n",
    "            tf.keras.layers.Dense(d_model),\n",
    "            tf.keras.layers.Dropout(dropout_rate)\n",
    "        ])\n",
    "        self.add = tf.keras.layers.Add()\n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.layer_norm(self.add([x, self.seq((x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = GlobalSelfAttentionLayer(num_heads = num_heads, key_dim = d_model, dropout_rate = dropout_rate)\n",
    "        self.ffn = FeedForwardLayer(d_model = d_model, dff = dff, dropout_rate = dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        return self.ffn(self.self_attention(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size = 9831, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEncoding(vocab_size = vocab_size, d_model = d_model)\n",
    "\n",
    "        self.enc_layers = [\n",
    "            EncoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout_rate = dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = CausalSelfAttentionLayer(num_heads = num_heads, key_dim = d_model, dropout_rate = dropout_rate)\n",
    "        self.cross_attention = CrossAttentionLayer(num_heads = num_heads, key_dim = d_model, dropout_rate = dropout_rate)\n",
    "        self.ffn = FeedForwardLayer(d_model, dff, dropout_rate = dropout_rate)\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.self_attention(x=x)\n",
    "        x = self.cross_attention(x=x, context=context)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size = 9831, dropout_rate = 0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.pos_embedding = PositionalEncoding(vocab_size = vocab_size, d_model = d_model)\n",
    "\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model = d_model, num_heads = num_heads, dff = dff, dropout_rate = dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, context):\n",
    "        x = self.pos_embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, *, num_layers, d_model, num_heads, dff, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, vocab_size = vocab_size, dropout_rate = dropout_rate)\n",
    "        self.decoder = Decoder(num_layers = num_layers, d_model = d_model, num_heads = num_heads, dff = dff, vocab_size = vocab_size, dropout_rate = dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # All inputs get passed to the first argument when keras.fit is called\n",
    "        context = inputs[0]\n",
    "        x = inputs[1]\n",
    "        context = self.encoder(context)\n",
    "        x = self.decoder(x, context)\n",
    "        logits = self.final_layer(x)\n",
    "\n",
    "        try:\n",
    "            del logits._keras_mask\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(num_layers = 4, d_model = 256, num_heads = 8, dff = 2048, vocab_size = 9831, dropout_rate = 0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 10:07:04.015245: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-09 10:07:04.107213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(24, 400, 9831), dtype=float32, numpy=\n",
       "array([[[ 3.83149274e-03, -1.45557284e-01, -2.56738245e-01, ...,\n",
       "         -8.07759631e-03, -1.19989939e-01,  4.08738218e-02],\n",
       "        [-1.17048360e-02,  1.56108811e-01, -2.81347752e-01, ...,\n",
       "          1.55833662e-01, -1.13874435e-01, -1.10019501e-02],\n",
       "        [-5.64794391e-02,  9.52024236e-02, -2.06767246e-01, ...,\n",
       "          2.91610118e-02,  1.18001483e-01, -6.10530265e-02],\n",
       "        ...,\n",
       "        [-8.46779570e-02, -3.38700503e-01, -3.06348622e-01, ...,\n",
       "          3.29118259e-02, -1.93358269e-02, -1.17504662e-02],\n",
       "        [-1.35689393e-01, -2.03048468e-01, -1.35522529e-01, ...,\n",
       "          5.91641851e-02, -1.49230789e-02, -8.11166242e-02],\n",
       "        [-7.75438175e-02, -2.49089167e-01, -3.37276161e-01, ...,\n",
       "          2.48936787e-01, -3.29920292e-01, -9.55722388e-03]],\n",
       "\n",
       "       [[-6.47288933e-02,  3.51304322e-01, -3.07557851e-01, ...,\n",
       "          1.16411693e-01, -1.30938798e-01, -1.25254810e-01],\n",
       "        [ 3.21624801e-04,  7.13333637e-02, -2.33311534e-01, ...,\n",
       "         -4.71835025e-03,  6.30836859e-02, -1.18605867e-02],\n",
       "        [ 7.39475265e-02,  2.98394948e-01,  8.85767266e-02, ...,\n",
       "          2.63463762e-02,  5.48863448e-02,  1.41973943e-01],\n",
       "        ...,\n",
       "        [-8.01720843e-02, -1.15089566e-01, -2.78782010e-01, ...,\n",
       "          1.80274267e-02, -1.35252312e-01, -1.64212734e-02],\n",
       "        [ 1.26830623e-01, -3.06403816e-01, -7.12222084e-02, ...,\n",
       "          1.02310188e-01, -1.83076277e-01, -1.23713553e-01],\n",
       "        [-2.54594952e-01, -1.01005379e-02, -1.81073770e-01, ...,\n",
       "          3.54890227e-01, -3.72675806e-01, -5.18651903e-01]],\n",
       "\n",
       "       [[-7.75943883e-03,  8.16309750e-02, -2.38738507e-01, ...,\n",
       "          4.68314737e-02,  8.94099772e-02, -9.65706855e-02],\n",
       "        [-8.13316852e-02, -4.36078757e-02, -2.87852734e-01, ...,\n",
       "          6.90445229e-02, -2.47309133e-01,  1.12590127e-01],\n",
       "        [-1.12286448e-01, -7.96883460e-03, -2.64872372e-01, ...,\n",
       "          9.09656584e-02,  1.31055012e-01,  2.50409581e-02],\n",
       "        ...,\n",
       "        [-1.54601336e-01, -2.67152876e-01,  5.43512814e-02, ...,\n",
       "          2.77393103e-01, -2.05078170e-01, -3.59810352e-01],\n",
       "        [-7.52651617e-02, -9.83750373e-02, -3.12527776e-01, ...,\n",
       "          9.18330178e-02, -1.09584533e-01, -2.19264910e-01],\n",
       "        [-1.68253139e-01, -1.12597816e-01, -4.63093035e-02, ...,\n",
       "          1.12837162e-02, -3.57892722e-01, -1.45561174e-01]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 7.03239888e-02,  5.08253314e-02, -8.88241753e-02, ...,\n",
       "          5.31781800e-02,  3.35510708e-02, -1.14790797e-01],\n",
       "        [ 5.80350459e-02, -2.06795204e-02, -3.07041854e-01, ...,\n",
       "          1.55725211e-01, -2.82037377e-01, -1.65753514e-01],\n",
       "        [-8.07472914e-02, -1.00420311e-01, -3.36082458e-01, ...,\n",
       "          1.17059015e-02, -8.04538429e-02, -1.96327612e-01],\n",
       "        ...,\n",
       "        [-2.36738235e-01, -1.94947436e-01, -1.48376137e-01, ...,\n",
       "          7.24700093e-02, -4.44580689e-02, -2.08044574e-01],\n",
       "        [-2.05318287e-01, -1.11894257e-01, -4.32666354e-02, ...,\n",
       "          1.77006215e-01, -2.16568902e-01, -1.06568731e-01],\n",
       "        [-3.15796107e-01, -1.76434860e-01, -9.67993587e-02, ...,\n",
       "          1.63827509e-01, -2.07114115e-01, -3.36969525e-01]],\n",
       "\n",
       "       [[ 1.41504869e-01, -1.82310324e-02, -1.44365400e-01, ...,\n",
       "          1.03710823e-01,  5.25450557e-02, -2.30493665e-01],\n",
       "        [ 6.64226860e-02, -1.58813465e-02, -4.80581760e-01, ...,\n",
       "          1.23935128e-02, -2.94391401e-02,  3.18635777e-02],\n",
       "        [ 1.77534297e-02,  1.02883957e-01, -3.24019849e-01, ...,\n",
       "          4.00360450e-02, -4.53932658e-02,  3.12413909e-02],\n",
       "        ...,\n",
       "        [-4.59547043e-02, -3.53851140e-01, -2.62559742e-01, ...,\n",
       "          1.57712266e-01, -2.52311379e-01, -3.18252832e-01],\n",
       "        [-2.33777333e-04, -3.79918635e-01, -7.91061670e-02, ...,\n",
       "         -8.70556161e-02, -1.48777477e-02, -9.09515843e-02],\n",
       "        [-1.66691080e-01, -1.36721596e-01, -1.30088925e-01, ...,\n",
       "          2.01109856e-01, -3.60210538e-01, -2.98281521e-01]],\n",
       "\n",
       "       [[-5.83681799e-02,  1.10314451e-01, -3.40961665e-01, ...,\n",
       "          8.33638757e-02,  6.26580641e-02, -1.23514317e-01],\n",
       "        [-1.19939588e-01,  1.33727819e-01, -3.24525267e-01, ...,\n",
       "         -2.02915683e-01, -2.17930123e-01,  1.62100077e-01],\n",
       "        [-8.28625113e-02, -7.44640604e-02, -3.01381916e-01, ...,\n",
       "          2.27345414e-02, -1.08551949e-01, -1.85168058e-01],\n",
       "        ...,\n",
       "        [-4.62014824e-02, -3.71557087e-01, -3.71189415e-03, ...,\n",
       "          1.28704473e-01, -1.62465483e-01, -1.63654685e-01],\n",
       "        [-9.67805311e-02, -3.54801476e-01, -1.67711470e-02, ...,\n",
       "          1.35603383e-01, -2.20099300e-01, -2.01075345e-01],\n",
       "        [-8.66877958e-02, -2.19754502e-01, -3.97193022e-02, ...,\n",
       "          1.90463960e-01, -2.77305484e-01, -1.90669149e-01]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer((inp, outp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  15138560  \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  23554816  \n",
      "                                                                 \n",
      " dense_16 (Dense)            multiple                  2526567   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41219943 (157.24 MB)\n",
      "Trainable params: 41219943 (157.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, dtype=tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1 = 0.9, beta_2 = 0.98, epsilon = 1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(label, pred):\n",
    "    mask = label != 0\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype = loss.dtype)\n",
    "    loss *= mask\n",
    "    loss = tf.reduce_sum(loss)/(tf.reduce_sum(mask) + tf.constant(1e-9))\n",
    "\n",
    "    return loss\n",
    "\n",
    "def masked_accuracy(label, pred):\n",
    "  pred = tf.argmax(pred, axis=2)\n",
    "  label = tf.cast(label, pred.dtype)\n",
    "  try:\n",
    "    match = (label == pred)\n",
    "  except:\n",
    "     match = 0\n",
    "     print(label)\n",
    "     print(pred)\n",
    "\n",
    "  mask = label != 0\n",
    "\n",
    "  match = match & mask\n",
    "\n",
    "  match = tf.cast(match, dtype=tf.float32)\n",
    "  mask = tf.cast(mask, dtype=tf.float32)\n",
    "  return tf.reduce_sum(match)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.compile(loss = masked_loss, optimizer = optimizer, metrics = [masked_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 10:07:23.365985: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2 of 10000\n",
      "2023-07-09 10:07:34.040137: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 9 of 10000\n",
      "2023-07-09 10:07:41.506308: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2997 of 10000\n",
      "2023-07-09 10:07:51.508244: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 6926 of 10000\n",
      "2023-07-09 10:08:00.054330: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n",
      "2023-07-09 10:08:00.255309: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7fa62003bd20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-07-09 10:08:00.255339: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3090, Compute Capability 8.6\n",
      "2023-07-09 10:08:00.260212: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-07-09 10:08:00.365479: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     59/Unknown - 85s 370ms/step - loss: 9.0846 - masked_accuracy: 0.0197"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 10:08:30.023594: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 28219507200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  12432/Unknown - 3333s 263ms/step - loss: 1.4003 - masked_accuracy: 0.7722"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 11:02:37.856213: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 12619718400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20834/Unknown - 5522s 262ms/step - loss: 0.8450 - masked_accuracy: 0.8625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 11:39:07.188760: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 4409908842706861072\n",
      "2023-07-09 11:39:07.188794: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 1552193096531666608\n",
      "2023-07-09 11:39:07.188799: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 7095406078362705709\n",
      "2023-07-09 11:39:18.400641: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2030 of 10000\n",
      "2023-07-09 11:39:28.401824: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 6224 of 10000\n",
      "2023-07-09 11:39:37.143089: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n",
      "2023-07-09 11:40:40.276462: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13103491200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20834/20834 [==============================] - 6121s 291ms/step - loss: 0.8450 - masked_accuracy: 0.8625 - val_loss: 0.0187 - val_masked_accuracy: 0.9970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 11:49:06.067617: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 9674532784807802747\n",
      "2023-07-09 11:49:06.067664: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 5263979607194460627\n",
      "2023-07-09 11:49:06.067683: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 10050494555370863737\n",
      "2023-07-09 11:49:06.067688: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 12311708681543296831\n",
      "2023-07-09 11:49:06.067696: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 4359087690752753260\n",
      "2023-07-09 11:49:06.067703: I tensorflow/core/framework/local_rendezvous.cc:405] Local rendezvous recv item cancelled. Key hash: 6014387454676997960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fa7c01cf820>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit(training_batches, epochs = 1, validation_data = validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.save_weights('my_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 11:49:19.516630: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2 of 10000\n",
      "2023-07-09 11:49:31.453449: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 9 of 10000\n",
      "2023-07-09 11:49:37.097824: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 2296 of 10000\n",
      "2023-07-09 11:49:47.097350: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 6103 of 10000\n",
      "2023-07-09 11:49:57.082246: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   7107/Unknown - 1902s 261ms/step - loss: 0.0186 - masked_accuracy: 0.9970"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 12:20:49.476018: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 11281718400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  15306/Unknown - 4038s 261ms/step - loss: 0.0177 - masked_accuracy: 0.9972"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 12:56:25.138461: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 13103491200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20834/Unknown - 5477s 261ms/step - loss: 0.0173 - masked_accuracy: 0.9974"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 13:20:24.564636: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 4409908842706861072\n",
      "2023-07-09 13:20:24.564662: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 1552193096531666608\n",
      "2023-07-09 13:20:24.564667: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 7095406078362705709\n",
      "2023-07-09 13:20:34.691231: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 1220 of 10000\n",
      "2023-07-09 13:20:44.720928: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 5718 of 10000\n",
      "2023-07-09 13:20:54.692591: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 7604 of 10000\n",
      "2023-07-09 13:20:59.446018: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20834/20834 [==============================] - 6082s 290ms/step - loss: 0.0173 - masked_accuracy: 0.9974 - val_loss: 0.0145 - val_masked_accuracy: 0.9978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 13:30:28.763165: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 9674532784807802747\n",
      "2023-07-09 13:30:28.763192: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 5263979607194460627\n",
      "2023-07-09 13:30:28.763198: I tensorflow/core/framework/local_rendezvous.cc:409] Local rendezvous send item cancelled. Key hash: 12311708681543296831\n",
      "2023-07-09 13:30:40.478131: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 8971 of 10000\n",
      "2023-07-09 13:30:40.836172: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n",
      "2023-07-09 13:30:40.837326: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 1 of 10000\n",
      "2023-07-09 13:30:53.244138: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 5 of 10000\n",
      "2023-07-09 13:30:59.696698: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 617 of 10000\n",
      "2023-07-09 13:31:09.697033: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 4453 of 10000\n",
      "2023-07-09 13:31:19.696332: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] Filling up shuffle buffer (this may take a while): 8519 of 10000\n",
      "2023-07-09 13:31:23.363573: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4264/Unknown - 1165s 261ms/step - loss: 0.0156 - masked_accuracy: 0.9976"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   4267/Unknown - 1166s 261ms/step - loss: 0.0156 - masked_accuracy: 0.9976"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    transformer.fit(training_batches, epochs = 1, validation_data = validation_batches)\n",
    "    transformer.save_weights(f'my_checkpoint_{k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.Module):\n",
    "    def __init__(self, tokenizer, transformer, temperature = 0.5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transformer = transformer\n",
    "        self.temperature = temperature\n",
    "        skip_ids = [[1],[2],[3]]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put a -inf at each bad index.\n",
    "            values=[-float('inf')]*len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[9831])\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "    \n",
    "    def __call__(self, sentence, max_length=MAX_TOKENS):\n",
    "        sentence = tf.constant(\"startstorystart \" + sentence)\n",
    "        assert isinstance(sentence, tf.Tensor)\n",
    "        if len(sentence.shape) == 0:\n",
    "            sentence = sentence[tf.newaxis]\n",
    "        \n",
    "        sentence = self.tokenizer.tokenize(sentence).merge_dims(-2, -1).to_tensor()\n",
    "        print(sentence)\n",
    "        start_end = self.tokenizer.tokenize(['startstorystart endstoryend'])[0]\n",
    "        start = start_end[0][tf.newaxis]\n",
    "        end = start_end[1][tf.newaxis]\n",
    "\n",
    "        for i in tf.range(max_length):\n",
    "            predictions = self.transformer([sentence, sentence], training = False)\n",
    "            predictions = predictions [:, -1, :]\n",
    "            predictions = predictions / self.temperature\n",
    "            predictions = predictions + self.prediction_mask\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples = 1)\n",
    "            next_token = tokenizer.detokenize(predicted_id)\n",
    "            sentence = tf.concat([sentence, predicted_id], axis = -1)[:, (0-max_length):]\n",
    "            if predicted_id == end:\n",
    "                break\n",
    "        \n",
    "        text = tokenizer.detokenize(sentence)[0]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentext = TextGenerator(tokenizer, transformer, temperature = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   4   76   80   27  553  105  265   62 1740   13   35  260  330   77\n",
      "  1970  729   13]], shape=(1, 17), dtype=int64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-09 13:51:04.512155: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "2023-07-09 13:51:04.617080: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=string, numpy=\n",
       "array([b'a', b'story', b'about', b'about', b'i', b'am', b'making', b'am',\n",
       "       b'making', b'am', b'making', b'stuff', b'about', b'about',\n",
       "       b'about', b'about', b'i', b'am', b'am', b'am', b'am', b'am', b'am',\n",
       "       b'am', b'am', b'about', b'about', b'about', b'about', b'about',\n",
       "       b'about', b'about', b'about', b'i', b'am', b'am', b'am', b'am',\n",
       "       b'am', b'am', b'am', b'am', b'am', b'am', b'am', b'about',\n",
       "       b'about', b'about', b'about', b'about', b'about', b'about', b'i',\n",
       "       b'i', b'am', b'am', b'am', b'am', b'am', b'am', b'about', b'about',\n",
       "       b'about', b'about', b'about', b'about', b'i', b'am', b'am', b'am',\n",
       "       b'am', b'am', b'am', b'about', b'about', b'about', b'about',\n",
       "       b'about', b'about', b'about', b'i', b'am', b'am', b'am', b'am',\n",
       "       b'am', b'am', b'am', b'about', b'about', b'about', b'about',\n",
       "       b'about', b'about', b'about', b'i', b'am', b'am', b'am', b'am'],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gentext(\"This is a story about nothing in particular. I am making up random stuff.\", max_length = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f5c2c430340>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.load_weights('my_checkpoint_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
